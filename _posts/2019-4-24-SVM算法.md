---
layout:     post                    # 使用的布局（不需要改）
title:      SVM算法               # 标题 
date:       2019-4-23              # 时间
author:     王伟                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:
    - 机器学习算法	                               #标签
---

## SVM算法概述

![](/img/SVM1.png)

左图两条实线也可以实现分类效果，但决策边界离分类点很近，这样的模型的泛化能力较差。通常对于一个分类问题，决策边界可以有无数个，如何确定最优的边界，保证模型的泛化能力最强则是SVM的核心思想。

右图实线是使用SVM算法得出的决策边界，该模型的泛化能力强于左侧模型，位于虚线上的样本点被称为**支持向量(所有的样本点中到决策边界距离最小的样本点)**，**margin表示最小间隔的大小**。

假设现在有一个决策边界：

- 计算每个样本点到边界的距离，距离最小的样本点则为**支持向量**
- 最小间隔与margin成正比(注意区分间隔与margin)
- 决策边界可以有无数个，SVM算法则是找出`max margin`

## 硬间隔计算

- 间隔与距离不是同一个概念
- margin即为支持向量的样本点到决策边界的距离
- 所有样本点到决策边界的点的间隔>=支持向量的样本点到决策边界的间隔

点到直线的距离：

<a href="https://www.codecogs.com/eqnedit.php?latex=distance&space;=&space;\frac{|\omega^Tx&plus;b|}{||w||}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?distance&space;=&space;\frac{|\omega^Tx&plus;b|}{||w||}" title="distance = \frac{|\omega^Tx+b|}{||w||}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{cases}&space;max(margin)=max(\frac{|\omega^Tx&plus;b|}{||w||})\\&space;|\omega^Tx&plus;b|>=\gamma&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{cases}&space;max(margin)=max(\frac{|\omega^Tx&plus;b|}{||w||})\\&space;|\omega^Tx&plus;b|>=\gamma&space;\end{cases}" title="\begin{cases} max(margin)=max(\frac{|\omega^Tx+b|}{||w||})\\ |\omega^Tx+b|>=\gamma \end{cases}" /></a>

假设共有N个样本点，则约束条件有N个，如果将w参数相应缩放一定倍数，决策边界不变，将支持向量的样本点间隔缩放为1，则上面公式变为：

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{cases}&space;max(margin)=max(\frac{1}{||w||})\\&space;|\omega^Tx&plus;b|>=1&space;\end{cases}&space;\\&space;y_i=\begin{cases}&space;1,\omega^Tx&plus;b>0\\&space;-1,\omega^Tx&plus;b>0<0&space;\end{cases}&space;\\&space;|\omega^Tx&plus;b|=y_i(\omega^Tx&plus;b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{cases}&space;max(margin)=max(\frac{1}{||w||})\\&space;|\omega^Tx&plus;b|>=1&space;\end{cases}&space;\\&space;y_i=\begin{cases}&space;1,\omega^Tx&plus;b>0\\&space;-1,\omega^Tx&plus;b>0<0&space;\end{cases}&space;\\&space;|\omega^Tx&plus;b|=y_i(\omega^Tx&plus;b)" title="\begin{cases} max(margin)=max(\frac{1}{||w||})\\ |\omega^Tx+b|>=1 \end{cases} \\ y_i=\begin{cases} 1,\omega^Tx+b>0\\ -1,\omega^Tx+b>0<0 \end{cases} \\ |\omega^Tx+b|=y_i(\omega^Tx+b)" /></a>

问题转化为：

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{cases}&space;f(\omega)=min(\frac{1}{2}\omega^T\omega)\\&space;y_i(\omega^Tx&plus;b)>=1&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{cases}&space;f(\omega)=min(\frac{1}{2}\omega^T\omega)\\&space;y_i(\omega^Tx&plus;b)>=1&space;\end{cases}" title="\begin{cases} f(\omega)=min(\frac{1}{2}\omega^T\omega)\\ y_i(\omega^Tx+b)>=1 \end{cases}" /></a>

这是带不等式约束的极值问题，构建拉格朗日乘子

<a href="https://www.codecogs.com/eqnedit.php?latex=f(w,b,\lambda)=min[w,b](max[\lambda](\frac{1}{2}w^Tw&plus;\sum_{i=1}^N\lambda_i(1-y_i(\omega^Tx_i&plus;b))))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(w,b,\lambda)=min[w,b](max[\lambda](\frac{1}{2}w^Tw&plus;\sum_{i=1}^N\lambda_i(1-y_i(\omega^Tx_i&plus;b))))" title="f(w,b,\lambda)=min[w,b](max[\lambda](\frac{1}{2}w^Tw+\sum_{i=1}^N\lambda_i(1-y_i(\omega^Tx_i+b))))" /></a>

目标函数为凸函数，约束条件为线性，故该函数可以使用KKT条件求解

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{cases}&space;\frac{\partial{f(w,b,\lambda)}}{\partial{b}}=0\\&space;\frac{\partial{f(w,b,\lambda)}}{\partial{w}}=0\\&space;\frac{\partial{f(w,b,\lambda)}}{\partial{\lambda}}=0\\&space;\lambda_i(1-y_i(\omega^Tx_i&plus;b))=0\\&space;\lambda_i>=0\\&space;1-y_i(\omega^Tx_i&plus;b)<=0&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{cases}&space;\frac{\partial{f(w,b,\lambda)}}{\partial{b}}=0\\&space;\frac{\partial{f(w,b,\lambda)}}{\partial{w}}=0\\&space;\frac{\partial{f(w,b,\lambda)}}{\partial{\lambda}}=0\\&space;\lambda_i(1-y_i(\omega^Tx_i&plus;b))=0\\&space;\lambda_i>=0\\&space;1-y_i(\omega^Tx_i&plus;b)<=0&space;\end{cases}" title="\begin{cases} \frac{\partial{f(w,b,\lambda)}}{\partial{b}}=0\\ \frac{\partial{f(w,b,\lambda)}}{\partial{w}}=0\\ \frac{\partial{f(w,b,\lambda)}}{\partial{\lambda}}=0\\ \lambda_i(1-y_i(\omega^Tx_i+b))=0\\ \lambda_i>=0\\ 1-y_i(\omega^Tx_i+b)<=0 \end{cases}" /></a>

对b求偏导

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial{f(w,b,\lambda)}}{\partial{b}}=\sum_{i=1}^N\lambda_iy_i=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial{f(w,b,\lambda)}}{\partial{b}}=\sum_{i=1}^N\lambda_iy_i=0" title="\frac{\partial{f(w,b,\lambda)}}{\partial{b}}=\sum_{i=1}^N\lambda_iy_i=0" /></a>

对w求偏导

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial{f(w,b,\lambda)}}{\partial{w}}=w-\sum_{i=1}^N\lambda_iy_ix_i=0\\&space;w=\sum_{i=1}^N\lambda_iy_ix_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial{f(w,b,\lambda)}}{\partial{w}}=w-\sum_{i=1}^N\lambda_iy_ix_i=0\\&space;w=\sum_{i=1}^N\lambda_iy_ix_i" title="\frac{\partial{f(w,b,\lambda)}}{\partial{w}}=w-\sum_{i=1}^N\lambda_iy_ix_i=0\\ w=\sum_{i=1}^N\lambda_iy_ix_i" /></a>

这里的w和x_i都是向量，w为N个向量的线性组合，将w代入原函数

<a href="https://www.codecogs.com/eqnedit.php?latex=f(\lambda)=\frac{1}{2}w^Tw-\sum_{i=1}^N\lambda_iy_ix_i\sum_{j=1}^N\lambda_jy_jx_j^T&plus;\sum_{i=1}^N\lambda_i-b\sum_{i=1}^N\lambda_iy_i\\&space;=\frac{1}{2}\sum_{i=1}^N\lambda_iy_ix_i\sum_{j=1}^N\lambda_jy_jx_j^T-\sum_{i=1}^N\lambda_iy_ix_i\sum_{j=1}^N\lambda_jy_jx_j^T&plus;\sum_{i=1}^N\lambda_i\\&space;=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\lambda_i\lambda_jy_iy_jx_ix_j^T&plus;\sum_{i=1}^N\lambda_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(\lambda)=\frac{1}{2}w^Tw-\sum_{i=1}^N\lambda_iy_ix_i\sum_{j=1}^N\lambda_jy_jx_j^T&plus;\sum_{i=1}^N\lambda_i-b\sum_{i=1}^N\lambda_iy_i\\&space;=\frac{1}{2}\sum_{i=1}^N\lambda_iy_ix_i\sum_{j=1}^N\lambda_jy_jx_j^T-\sum_{i=1}^N\lambda_iy_ix_i\sum_{j=1}^N\lambda_jy_jx_j^T&plus;\sum_{i=1}^N\lambda_i\\&space;=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\lambda_i\lambda_jy_iy_jx_ix_j^T&plus;\sum_{i=1}^N\lambda_i" title="f(\lambda)=\frac{1}{2}w^Tw-\sum_{i=1}^N\lambda_iy_ix_i\sum_{j=1}^N\lambda_jy_jx_j^T+\sum_{i=1}^N\lambda_i-b\sum_{i=1}^N\lambda_iy_i\\ =\frac{1}{2}\sum_{i=1}^N\lambda_iy_ix_i\sum_{j=1}^N\lambda_jy_jx_j^T-\sum_{i=1}^N\lambda_iy_ix_i\sum_{j=1}^N\lambda_jy_jx_j^T+\sum_{i=1}^N\lambda_i\\ =-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\lambda_i\lambda_jy_iy_jx_ix_j^T+\sum_{i=1}^N\lambda_i" /></a>

即求f的最大值，转化成求最小值

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{cases}&space;g(\lambda)=-f(\lambda)=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\lambda_i\lambda_jy_iy_jx_ix_j^T-\sum_{i=1}^N\lambda_i\\&space;\lambda_i>=0\\&space;\sum_{i=1}^N\lambda_iy_i=0&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{cases}&space;g(\lambda)=-f(\lambda)=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\lambda_i\lambda_jy_iy_jx_ix_j^T-\sum_{i=1}^N\lambda_i\\&space;\lambda_i>=0\\&space;\sum_{i=1}^N\lambda_iy_i=0&space;\end{cases}" title="\begin{cases} g(\lambda)=-f(\lambda)=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\lambda_i\lambda_jy_iy_jx_ix_j^T-\sum_{i=1}^N\lambda_i\\ \lambda_i>=0\\ \sum_{i=1}^N\lambda_iy_i=0 \end{cases}" /></a>

此时转为一个参数的条件极值，使用拉格朗日乘子法直接求解

结果代入w即可求出w，最后求b，最优决策面一定处于-1和+1支持向量的中间位置，即：

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{cases}&space;w^Tx_{sv-}&plus;(b-)=0\\&space;w^Tx_{sv&plus;}&plus;(b&plus;)=0\\&space;b^*=\frac{(b-)&plus;(b&plus;)}{2}\\&space;b-=-max_{y_i=-1}(w^{*T}x_i)\\&space;b&plus;=-min_{y_i=&plus;1}(w^{*T}x_i)&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{cases}&space;w^Tx_{sv-}&plus;(b-)=0\\&space;w^Tx_{sv&plus;}&plus;(b&plus;)=0\\&space;b^*=\frac{(b-)&plus;(b&plus;)}{2}\\&space;b-=-max_{y_i=-1}(w^{*T}x_i)\\&space;b&plus;=-min_{y_i=&plus;1}(w^{*T}x_i)&space;\end{cases}" title="\begin{cases} w^Tx_{sv-}+(b-)=0\\ w^Tx_{sv+}+(b+)=0\\ b^*=\frac{(b-)+(b+)}{2}\\ b-=-max_{y_i=-1}(w^{*T}x_i)\\ b+=-min_{y_i=+1}(w^{*T}x_i) \end{cases}" /></a>

求解出b。

## 软间隔

如果样本中存在噪声点，即离最优平面很近的点，硬间隔为了满足此类点，一定会使得margin值变小，模型的泛化能力降低。软间隔的损失函数及约束条件如下

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{cases}&space;f=\frac{1}{2}w^Tw&plus;c\sum_{i=1}^N\epsilon_i\\&space;y_i(w^Tx_i&plus;b)>=1-\epsilon_i\\&space;\epsilon_i>=0&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{cases}&space;f=\frac{1}{2}w^Tw&plus;c\sum_{i=1}^N\epsilon_i\\&space;y_i(w^Tx_i&plus;b)>=1-\epsilon_i\\&space;\epsilon_i>=0&space;\end{cases}" title="\begin{cases} f=\frac{1}{2}w^Tw+c\sum_{i=1}^N\epsilon_i\\ y_i(w^Tx_i+b)>=1-\epsilon_i\\ \epsilon_i>=0 \end{cases}" /></a>

- 样本间隔>=1，这类样本属于正确样本，由于f要最小化，所以`\epsilon`必然取值为0，也就是正确的样本对模型无影响
- 样本间隔<1，这类样本输入错误样本，此时`\epsilon`必然为实际偏离的间隔，也就是犯了多大的错误

> 通俗的讲，软间隔代表可以容忍的样本错误程度，c代表容忍程度的大小

