---
layout:     post                    # 使用的布局（不需要改）
title:      线性回归算法               # 标题 
subtitle:   微分法和梯度下降法    #副标题
date:       2018-11-24              # 时间
author:     王伟                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 机器学习算法
    - 线性回归
---

现有训练样本集

<a href="https://www.codecogs.com/eqnedit.php?latex=\left[&space;\begin{array}{ccccc|c}&space;x_1^1&space;&&space;x_1^2&space;&&space;x_1^3&space;&&space;...&space;&&space;x_1^n&space;&&space;y_1&space;\\&space;x_2^1&space;&&space;x_2^2&space;&&space;x_2^3&space;&&space;...&space;&&space;x_2^n&space;&&space;y_2&space;\\&space;x_3^1&space;&&space;x_3^2&space;&&space;x_3^3&space;&&space;...&space;&&space;x_3^n&space;&&space;y_3&space;\\&space;...&space;&&space;...&space;&&space;...&space;&&space;...&space;&&space;...&space;&&space;...&space;\\&space;x_m^1&space;&&space;x_m^2&space;&&space;x_m^3&space;&&space;...&space;&&space;x_m^n&space;&&space;y_m&space;\end{array}&space;\right]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\left[&space;\begin{array}{ccccc|c}&space;x_1^1&space;&&space;x_1^2&space;&&space;x_1^3&space;&&space;...&space;&&space;x_1^n&space;&&space;y_1&space;\\&space;x_2^1&space;&&space;x_2^2&space;&&space;x_2^3&space;&&space;...&space;&&space;x_2^n&space;&&space;y_2&space;\\&space;x_3^1&space;&&space;x_3^2&space;&&space;x_3^3&space;&&space;...&space;&&space;x_3^n&space;&&space;y_3&space;\\&space;...&space;&&space;...&space;&&space;...&space;&&space;...&space;&&space;...&space;&&space;...&space;\\&space;x_m^1&space;&&space;x_m^2&space;&&space;x_m^3&space;&&space;...&space;&&space;x_m^n&space;&&space;y_m&space;\end{array}&space;\right]" title="\left[ \begin{array}{ccccc|c} x_1^1 & x_1^2 & x_1^3 & ... & x_1^n & y_1 \\ x_2^1 & x_2^2 & x_2^3 & ... & x_2^n & y_2 \\ x_3^1 & x_3^2 & x_3^3 & ... & x_3^n & y_3 \\ ... & ... & ... & ... & ... & ... \\ x_m^1 & x_m^2 & x_m^3 & ... & x_m^n & y_m \end{array} \right]" /></a>

样本共有m条数据，n个特征，x表示样本的特征，y表示结果数据

单条样本的预测模型(线性模型)：

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{y}=\theta_0&plus;\theta_1x_1&plus;\theta_2x_2&plus;\theta_3x_3&plus;...&plus;\theta_nx_n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{y}=\theta_0&plus;\theta_1x_1&plus;\theta_2x_2&plus;\theta_3x_3&plus;...&plus;\theta_nx_n" title="\hat{y}=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+...+\theta_nx_n" /></a>

扩展到整个训练集：

<a href="https://www.codecogs.com/eqnedit.php?latex=\left[\begin{matrix}&space;1&space;&&space;x_1^1&space;&&space;x_1^2&space;&&space;x_1^3&space;&&space;...&space;&&space;x_1^n\\&space;1&space;&&space;x_2^1&space;&&space;x_2^2&space;&&space;x_2^3&space;&&space;...&space;&&space;x_2^n\\&space;1&space;&&space;x_3^1&space;&&space;x_3^2&space;&&space;x_3^3&space;&&space;...&space;&&space;x_3^n\\&space;...&space;&&space;...&space;&&space;...&space;&&space;...&space;&&space;...&space;&&space;...\\&space;1&space;&&space;x_m^1&space;&&space;x_m^2&space;&&space;x_m^3&space;&&space;...&space;&&space;x_m^n&space;\end{matrix}\right]&space;\left[\begin{matrix}&space;\theta_0\\&space;\theta_1\\&space;\theta_2\\&space;...\\&space;\theta_n&space;\end{matrix}\right]=&space;\left[\begin{matrix}&space;\hat{y_1}\\&space;\hat{y_2}\\&space;\hat{y_3}\\&space;...\\&space;\hat{y_m}&space;\end{matrix}\right]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\left[\begin{matrix}&space;1&space;&&space;x_1^1&space;&&space;x_1^2&space;&&space;x_1^3&space;&&space;...&space;&&space;x_1^n\\&space;1&space;&&space;x_2^1&space;&&space;x_2^2&space;&&space;x_2^3&space;&&space;...&space;&&space;x_2^n\\&space;1&space;&&space;x_3^1&space;&&space;x_3^2&space;&&space;x_3^3&space;&&space;...&space;&&space;x_3^n\\&space;...&space;&&space;...&space;&&space;...&space;&&space;...&space;&&space;...&space;&&space;...\\&space;1&space;&&space;x_m^1&space;&&space;x_m^2&space;&&space;x_m^3&space;&&space;...&space;&&space;x_m^n&space;\end{matrix}\right]&space;\left[\begin{matrix}&space;\theta_0\\&space;\theta_1\\&space;\theta_2\\&space;...\\&space;\theta_n&space;\end{matrix}\right]=&space;\left[\begin{matrix}&space;\hat{y_1}\\&space;\hat{y_2}\\&space;\hat{y_3}\\&space;...\\&space;\hat{y_m}&space;\end{matrix}\right]" title="\left[\begin{matrix} 1 & x_1^1 & x_1^2 & x_1^3 & ... & x_1^n\\ 1 & x_2^1 & x_2^2 & x_2^3 & ... & x_2^n\\ 1 & x_3^1 & x_3^2 & x_3^3 & ... & x_3^n\\ ... & ... & ... & ... & ... & ...\\ 1 & x_m^1 & x_m^2 & x_m^3 & ... & x_m^n \end{matrix}\right] \left[\begin{matrix} \theta_0\\ \theta_1\\ \theta_2\\ ...\\ \theta_n \end{matrix}\right]= \left[\begin{matrix} \hat{y_1}\\ \hat{y_2}\\ \hat{y_3}\\ ...\\ \hat{y_m} \end{matrix}\right]" /></a>

训练目标：预测结果与实际结果误差最小

## 误差的似然函数推导

前提：误差符合高斯分布，事实上大部分情况确是如此

高斯函数：
$$
f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

$\mu$：数学期望

$\sigma$：样本的标准差

![](/img/高斯分布.png)

模型的误差为x轴的值，出现的概率为y轴，求出似然函数如下：

<a href="https://www.codecogs.com/eqnedit.php?latex=p(\epsilon)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\epsilon-\mu)^2}{2\sigma^2}}\\&space;f=\prod&space;p(\epsilon)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(\epsilon)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\epsilon-\mu)^2}{2\sigma^2}}\\&space;f=\prod&space;p(\epsilon)" title="p(\epsilon)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\epsilon-\mu)^2}{2\sigma^2}}\\ f=\prod p(\epsilon)" /></a>

f就是样本误差满足当前情况的概率，现在问题变为求f的最大值

<a href="https://www.codecogs.com/eqnedit.php?latex=F=\ln(f)=\sum\ln(p(\epsilon))=\sum&space;p(\epsilon)=\sum{[\ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{(\epsilon-\mu)^2}{2\sigma^2}]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?F=\ln(f)=\sum\ln(p(\epsilon))=\sum&space;p(\epsilon)=\sum{[\ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{(\epsilon-\mu)^2}{2\sigma^2}]}" title="F=\ln(f)=\sum\ln(p(\epsilon))=\sum p(\epsilon)=\sum{[\ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{(\epsilon-\mu)^2}{2\sigma^2}]}" /></a>

转化为求-F的最小值，去掉常数项之后问题转变为求

<a href="https://www.codecogs.com/eqnedit.php?latex=\sum{(\epsilon-\mu)^2}=\sum{(\hat{y}-y-\mu)^2}\Rightarrow\sum{(\hat{y}-y)^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum{(\epsilon-\mu)^2}=\sum{(\hat{y}-y-\mu)^2}\Rightarrow\sum{(\hat{y}-y)^2}" title="\sum{(\epsilon-\mu)^2}=\sum{(\hat{y}-y-\mu)^2}\Rightarrow\sum{(\hat{y}-y)^2}" /></a>

的最小值

目标函数：

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align}&space;J(\theta)&=(\hat{y}-y)^T(\hat{y}-y)\\&space;&=(X\theta-y)^T(X\theta-y)\\&space;&=((X\theta)^T-y^T)(X\theta-y)\\&space;&=(X\theta)^TX\theta-(X\theta)^Ty-y^TX\theta&plus;y^Ty&space;\end{align}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align}&space;J(\theta)&=(\hat{y}-y)^T(\hat{y}-y)\\&space;&=(X\theta-y)^T(X\theta-y)\\&space;&=((X\theta)^T-y^T)(X\theta-y)\\&space;&=(X\theta)^TX\theta-(X\theta)^Ty-y^TX\theta&plus;y^Ty&space;\end{align}" title="\begin{align} J(\theta)&=(\hat{y}-y)^T(\hat{y}-y)\\ &=(X\theta-y)^T(X\theta-y)\\ &=((X\theta)^T-y^T)(X\theta-y)\\ &=(X\theta)^TX\theta-(X\theta)^Ty-y^TX\theta+y^Ty \end{align}" /></a>

对J进行求导：

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align}&space;\frac{\partial{J(\theta)}}{\partial{\theta}}&=\frac{\partial[(X\theta)^TX\theta-(X\theta)^Ty-y^TX\theta&plus;y^Ty]}{\partial\theta}\\&space;&=\frac{\partial(X\theta)^T}{\partial\theta}X\theta&plus;\frac{\partial(X\theta)^T}{\partial\theta}X\theta-\frac{\partial&space;\theta^TX^Ty}{\partial\theta}-\frac{\partial&space;(X^Ty)^T}{\partial\theta}\theta-\frac{\partial&space;\theta^T}{\partial\theta}X^Ty\\&space;&=X^TX\theta&plus;X^TX\theta-X^Ty-0-X^Ty\\&space;&=2X^TX\theta-2X^Ty&space;\end{align}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align}&space;\frac{\partial{J(\theta)}}{\partial{\theta}}&=\frac{\partial[(X\theta)^TX\theta-(X\theta)^Ty-y^TX\theta&plus;y^Ty]}{\partial\theta}\\&space;&=\frac{\partial(X\theta)^T}{\partial\theta}X\theta&plus;\frac{\partial(X\theta)^T}{\partial\theta}X\theta-\frac{\partial&space;\theta^TX^Ty}{\partial\theta}-\frac{\partial&space;(X^Ty)^T}{\partial\theta}\theta-\frac{\partial&space;\theta^T}{\partial\theta}X^Ty\\&space;&=X^TX\theta&plus;X^TX\theta-X^Ty-0-X^Ty\\&space;&=2X^TX\theta-2X^Ty&space;\end{align}" title="\begin{align} \frac{\partial{J(\theta)}}{\partial{\theta}}&=\frac{\partial[(X\theta)^TX\theta-(X\theta)^Ty-y^TX\theta+y^Ty]}{\partial\theta}\\ &=\frac{\partial(X\theta)^T}{\partial\theta}X\theta+\frac{\partial(X\theta)^T}{\partial\theta}X\theta-\frac{\partial \theta^TX^Ty}{\partial\theta}-\frac{\partial (X^Ty)^T}{\partial\theta}\theta-\frac{\partial \theta^T}{\partial\theta}X^Ty\\ &=X^TX\theta+X^TX\theta-X^Ty-0-X^Ty\\ &=2X^TX\theta-2X^Ty \end{align}" /></a>

## 微分法求最优解

在机器学习中，最小值总是在偏导为0的地方

<a href="https://www.codecogs.com/eqnedit.php?latex=2X^TX\theta-2X^Ty=0\Rightarrow&space;X^TX\theta=X^Ty" target="_blank"><img src="https://latex.codecogs.com/gif.latex?2X^TX\theta-2X^Ty=0\Rightarrow&space;X^TX\theta=X^Ty" title="2X^TX\theta-2X^Ty=0\Rightarrow X^TX\theta=X^Ty" /></a>

得出的结论与[矩阵投影](https://wisew.github.io/2018/11/20/最小二乘法/)得出的结果一致  
当样本集特征很多的额时候，求解方程的难度很大，所以下面介绍更通用的方法，`梯度下降`，它不仅可以解决线性回归的问题，同样也能求解更复杂的回归方程。  
## 梯度下降求最优解
> 梯度反应的是使目标函数值减小的方向，如下图的g方向为目标函数的一个梯度方向 

如下图所示，A为初始点，a代表切线方向，当a的斜率<0时，y是单调减的，反之则单调增，可以得出，A的行走方向应与斜率的正负情况相反。  
A的行进方向可以表示为

![](/img/梯度.png)

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align}&space;\theta&space;&=\theta_0-\eta\frac{\partial&space;J(\theta)}{\partial&space;\theta}\\&space;&=\theta_0-\eta(2X^TX\theta-2X^Ty)&space;\end{align}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align}&space;\theta&space;&=\theta_0-\eta\frac{\partial&space;J(\theta)}{\partial&space;\theta}\\&space;&=\theta_0-\eta(2X^TX\theta-2X^Ty)&space;\end{align}" title="\begin{align} \theta &=\theta_0-\eta\frac{\partial J(\theta)}{\partial \theta}\\ &=\theta_0-\eta(2X^TX\theta-2X^Ty) \end{align}" /></a>

![](/img/梯度下降的参数说明)