---
layout:     post                    # 使用的布局（不需要改）
title:      逻辑回归算法               # 标题 
subtitle:   理论基础       #副标题
date:       2018-12-01              # 时间
author:     王伟                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 机器学习算法
    - 逻辑回归
---

## sigmod函数
![](/img/sigmode曲线图像.png)

sigmod函数的表达式  
$$
p=\frac{1}{1+e^{-z}}
$$
函数的取值范围为[-1,1]

作用：实现了真实值和概率之间的互转

## 逻辑回归算法

设预测值为y,y的取值为1或0

则对于每一条数据，概率为:

$$
\begin{cases}
p_i=\frac{1}{1+e^{-z}} \\
z=\theta^Tx_i \\
P_i=p_i^{y_i}*(1-p_i)^{1-y_i}
\end{cases}
$$
P表示的是每一条样本的最终概率值

求解似然函数：
$$
L(\theta)=\prod P_i \\
\begin{align} J(\theta)=-ln(L(\theta))&=-\sum ln(P_i)\\
&=-\sum [y_i ln(p_i)+(1-y_i)ln(1-p_i)]
\end{align} \\
$$
$$
\begin{cases}
\begin{align}
\frac{\partial J(\theta)}{\partial \theta}&=-\sum(y_i \frac{1}{p_i} \frac{\partial p_i}{\partial \theta}-\frac{1}{1-p_i}\frac{\partial p_i}{\partial \theta}+y_i \frac{1}{1-p_i}\frac{\partial p_i}{\partial \theta})\\
&=\sum(\frac{1-y_i}{1-p_i}-\frac{y_i}{p_i})\frac{\partial p_i}{\partial \theta}\\
&=\sum\frac{p_i-y_i}{(1-p_i)p_i}\frac{\partial p_i}{\partial \theta}\\
\frac{\partial p_i}{\partial \theta}&=-(\frac{1}{1+e^{-z}})^2e^{-z}[-\frac{\partial (\theta^Tx_i)}{\partial \theta}]\\
&=(\frac{1}{1+e^{-z}})^2e^{-z}x_i
\end{align}
\end{cases}
$$

转化为矩阵并去除常数项后得到梯度：

$$
\nabla=X^T
\left[
\begin{matrix}
p_1-y_1 \\
p_2-y_2 \\
... \\
p_m-y_m
\end{matrix}
\right]
=X^Tp-X^Ty
$$

比较一下线性回归中的梯度

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{align}&space;\frac{\partial{J(\theta)}}{\partial{\theta}}&=\frac{\partial[(X\theta)^TX\theta-(X\theta)^Ty-y^TX\theta&plus;y^Ty]}{\partial\theta}\\&space;&=\frac{\partial(X\theta)^T}{\partial\theta}X\theta&plus;\frac{\partial(X\theta)^T}{\partial\theta}X\theta-\frac{\partial&space;\theta^TX^Ty}{\partial\theta}-\frac{\partial&space;(X^Ty)^T}{\partial\theta}\theta-\frac{\partial&space;\theta^T}{\partial\theta}X^Ty\\&space;&=X^TX\theta&plus;X^TX\theta-X^Ty-0-X^Ty\\&space;&=2X^TX\theta-2X^Ty&space;\end{align}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{align}&space;\frac{\partial{J(\theta)}}{\partial{\theta}}&=\frac{\partial[(X\theta)^TX\theta-(X\theta)^Ty-y^TX\theta&plus;y^Ty]}{\partial\theta}\\&space;&=\frac{\partial(X\theta)^T}{\partial\theta}X\theta&plus;\frac{\partial(X\theta)^T}{\partial\theta}X\theta-\frac{\partial&space;\theta^TX^Ty}{\partial\theta}-\frac{\partial&space;(X^Ty)^T}{\partial\theta}\theta-\frac{\partial&space;\theta^T}{\partial\theta}X^Ty\\&space;&=X^TX\theta&plus;X^TX\theta-X^Ty-0-X^Ty\\&space;&=2X^TX\theta-2X^Ty&space;\end{align}" title="\begin{align} \frac{\partial{J(\theta)}}{\partial{\theta}}&=\frac{\partial[(X\theta)^TX\theta-(X\theta)^Ty-y^TX\theta+y^Ty]}{\partial\theta}\\ &=\frac{\partial(X\theta)^T}{\partial\theta}X\theta+\frac{\partial(X\theta)^T}{\partial\theta}X\theta-\frac{\partial \theta^TX^Ty}{\partial\theta}-\frac{\partial (X^Ty)^T}{\partial\theta}\theta-\frac{\partial \theta^T}{\partial\theta}X^Ty\\ &=X^TX\theta+X^TX\theta-X^Ty-0-X^Ty\\ &=2X^TX\theta-2X^Ty \end{align}" />

逻辑回归中，很难直接求解出最优解，所以此时可采取梯度下降的方式进行求解

