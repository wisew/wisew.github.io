---
layout:     post                    # 使用的布局（不需要改）
title:      逻辑回归算法               # 标题 
subtitle:   理论基础       #副标题
date:       2018-12-01              # 时间
author:     王伟                     # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 机器学习算法
    - 逻辑回归
---

## sigmod函数
![](/img/sigmode曲线图像.png)

sigmod函数的表达式  

<a href="https://www.codecogs.com/eqnedit.php?latex=p=\frac{1}{1&plus;e^{-z}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p=\frac{1}{1&plus;e^{-z}}" title="p=\frac{1}{1+e^{-z}}" /></a>

函数的取值范围为(0,1)

作用：实现了真实值和概率之间的互转

## 逻辑回归算法

设预测值为y,y的取值为1或0

则对于每一条数据，概率为:

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{cases}&space;p_i=\frac{1}{1&plus;e^{-z}}&space;\\&space;z=\theta^Tx_i&space;\\&space;P_i=p_i^{y_i}*(1-p_i)^{1-y_i}&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{cases}&space;p_i=\frac{1}{1&plus;e^{-z}}&space;\\&space;z=\theta^Tx_i&space;\\&space;P_i=p_i^{y_i}*(1-p_i)^{1-y_i}&space;\end{cases}" title="\begin{cases} p_i=\frac{1}{1+e^{-z}} \\ z=\theta^Tx_i \\ P_i=p_i^{y_i}*(1-p_i)^{1-y_i} \end{cases}" /></a>

P表示的是每一条样本的最终概率值

求解似然函数：

<a href="https://www.codecogs.com/eqnedit.php?latex=L(\theta)=\prod&space;P_i&space;\\&space;\begin{align}&space;J(\theta)=-ln(L(\theta))&=-\sum&space;ln(P_i)\\&space;&=-\sum&space;[y_i&space;ln(p_i)&plus;(1-y_i)ln(1-p_i)]&space;\end{align}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L(\theta)=\prod&space;P_i&space;\\&space;\begin{align}&space;J(\theta)=-ln(L(\theta))&=-\sum&space;ln(P_i)\\&space;&=-\sum&space;[y_i&space;ln(p_i)&plus;(1-y_i)ln(1-p_i)]&space;\end{align}" title="L(\theta)=\prod P_i \\ \begin{align} J(\theta)=-ln(L(\theta))&=-\sum ln(P_i)\\ &=-\sum [y_i ln(p_i)+(1-y_i)ln(1-p_i)] \end{align}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\begin{cases}&space;\begin{align}&space;\frac{\partial&space;J(\theta)}{\partial&space;\theta}&=-\sum(y_i&space;\frac{1}{p_i}&space;\frac{\partial&space;p_i}{\partial&space;\theta}-\frac{1}{1-p_i}\frac{\partial&space;p_i}{\partial&space;\theta}&plus;y_i&space;\frac{1}{1-p_i}\frac{\partial&space;p_i}{\partial&space;\theta})\\&space;&=\sum(\frac{1-y_i}{1-p_i}-\frac{y_i}{p_i})\frac{\partial&space;p_i}{\partial&space;\theta}\\&space;&=\sum\frac{p_i-y_i}{(1-p_i)p_i}\frac{\partial&space;p_i}{\partial&space;\theta}\\&space;\frac{\partial&space;p_i}{\partial&space;\theta}&=-(\frac{1}{1&plus;e^{-z}})^2e^{-z}[-\frac{\partial&space;(\theta^Tx_i)}{\partial&space;\theta}]\\&space;&=(\frac{1}{1&plus;e^{-z}})^2e^{-z}x_i&space;\end{align}&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\begin{cases}&space;\begin{align}&space;\frac{\partial&space;J(\theta)}{\partial&space;\theta}&=-\sum(y_i&space;\frac{1}{p_i}&space;\frac{\partial&space;p_i}{\partial&space;\theta}-\frac{1}{1-p_i}\frac{\partial&space;p_i}{\partial&space;\theta}&plus;y_i&space;\frac{1}{1-p_i}\frac{\partial&space;p_i}{\partial&space;\theta})\\&space;&=\sum(\frac{1-y_i}{1-p_i}-\frac{y_i}{p_i})\frac{\partial&space;p_i}{\partial&space;\theta}\\&space;&=\sum\frac{p_i-y_i}{(1-p_i)p_i}\frac{\partial&space;p_i}{\partial&space;\theta}\\&space;\frac{\partial&space;p_i}{\partial&space;\theta}&=-(\frac{1}{1&plus;e^{-z}})^2e^{-z}[-\frac{\partial&space;(\theta^Tx_i)}{\partial&space;\theta}]\\&space;&=(\frac{1}{1&plus;e^{-z}})^2e^{-z}x_i&space;\end{align}&space;\end{cases}" title="\begin{cases} \begin{align} \frac{\partial J(\theta)}{\partial \theta}&=-\sum(y_i \frac{1}{p_i} \frac{\partial p_i}{\partial \theta}-\frac{1}{1-p_i}\frac{\partial p_i}{\partial \theta}+y_i \frac{1}{1-p_i}\frac{\partial p_i}{\partial \theta})\\ &=\sum(\frac{1-y_i}{1-p_i}-\frac{y_i}{p_i})\frac{\partial p_i}{\partial \theta}\\ &=\sum\frac{p_i-y_i}{(1-p_i)p_i}\frac{\partial p_i}{\partial \theta}\\ \frac{\partial p_i}{\partial \theta}&=-(\frac{1}{1+e^{-z}})^2e^{-z}[-\frac{\partial (\theta^Tx_i)}{\partial \theta}]\\ &=(\frac{1}{1+e^{-z}})^2e^{-z}x_i \end{align} \end{cases}" /></a>

转化为矩阵得到梯度：

<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;\begin{cases}&space;\nabla=X^T&space;\left[&space;\begin{matrix}&space;\alpha_1(p_1-y_1)&space;\\&space;\alpha_2(p_2-y_2)&space;\\&space;...&space;\\&space;\alpha_m(p_m-y_m)&space;\end{matrix}&space;\right]\\&space;\alpha=\frac{1}{(1-p_i)p_i}(\frac{1}{1&plus;e^{-z}})^2e^{-z}&space;\end{cases}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&space;\begin{cases}&space;\nabla=X^T&space;\left[&space;\begin{matrix}&space;\alpha_1(p_1-y_1)&space;\\&space;\alpha_2(p_2-y_2)&space;\\&space;...&space;\\&space;\alpha_m(p_m-y_m)&space;\end{matrix}&space;\right]\\&space;\alpha=\frac{1}{(1-p_i)p_i}(\frac{1}{1&plus;e^{-z}})^2e^{-z}&space;\end{cases}" title="\begin{cases} \nabla=X^T \left[ \begin{matrix} \alpha_1(p_1-y_1) \\ \alpha_2(p_2-y_2) \\ ... \\ \alpha_m(p_m-y_m) \end{matrix} \right]\\ \alpha=\frac{1}{(1-p_i)p_i}(\frac{1}{1+e^{-z}})^2e^{-z} \end{cases}" /></a>

- 系数可以转化为对角方阵,使用numpy中的dot求解

- 也可以直接使用numpy中的`*`进行求解